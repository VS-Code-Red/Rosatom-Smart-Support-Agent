{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0580ba69-2c66-4dc2-88e7-c31f9258f770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\masha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Импорт модулей\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Скачайте только раз, если не скачивали ранее\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^а-яёa-z0-9\\s]', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Загрузите обращения\n",
    "with open('../data/raw/Obrashcheniia.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Очистите обращения\n",
    "cleaned = [preprocess_text(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07105ef9-c09f-44ea-97f7-a1abb62899ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'прошу': 167\n",
      "'10': 161\n",
      "'требуется': 124\n",
      "'ошибка': 98\n",
      "'работает': 95\n",
      "'1с': 83\n",
      "'нужна': 67\n",
      "'заявка': 60\n",
      "'могу': 60\n",
      "'оформить': 59\n",
      "'добавить': 54\n",
      "'нужно': 53\n",
      "'документ': 48\n",
      "'формируется': 47\n",
      "'11': 47\n",
      "'заявке': 47\n",
      "'прошу оформить': 46\n",
      "'ндс': 45\n",
      "'документы': 44\n",
      "'доступ': 42\n",
      "'ошибку': 42\n",
      "'настроить': 40\n",
      "'повторно': 40\n",
      "'помощь': 40\n",
      "'12': 39\n",
      "'16': 38\n",
      "'выдать': 37\n",
      "'логи': 35\n",
      "'срочно': 33\n",
      "'права': 33\n",
      "'системе': 32\n",
      "'error': 30\n",
      "'работа': 30\n",
      "'01': 29\n",
      "'заявки': 28\n",
      "'проходит': 28\n",
      "'обновления': 28\n",
      "'15': 28\n",
      "'всё': 27\n",
      "'отпуск': 27\n",
      "'работы': 25\n",
      "'16 10': 25\n",
      "'период': 25\n",
      "'приложил': 25\n",
      "'обращение': 25\n",
      "'срок': 25\n",
      "'сетевой': 24\n",
      "'попытке': 24\n",
      "'работать': 24\n",
      "'диск': 24\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), max_features=1000)\n",
    "X = vectorizer.fit_transform(cleaned)\n",
    "sums = X.sum(axis=0)\n",
    "words_freq = [(word, sums[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for word, freq in words_freq[:50]:\n",
    "    print(f\"'{word}': {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78be0337-cdd6-4737-8f5f-3346ff0a3f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Готово! Ваши данные разметились по категориям и сохранены в training_data.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Загрузить файл правил из папки data/rules\n",
    "with open('../data/rules/rules.json', 'r', encoding='utf-8') as f:\n",
    "    rules = json.load(f)\n",
    "\n",
    "# 2. Функция для назначения категории на основе правил\n",
    "def assign_category(text):\n",
    "    # Для каждой категории и списка ключевых слов\n",
    "    for category, keywords in rules.items():\n",
    "        # Проверяем, есть ли в тексте любое ключевое слово\n",
    "        for kw in keywords:\n",
    "            if kw in text:\n",
    "                return category\n",
    "    # Если ни одно ключевое слово не найдено, категория 'other'\n",
    "    return 'other'\n",
    "\n",
    "# 3. Применить функцию ко всем очищенным текстам\n",
    "# Замените 'cleaned' на ваш список очищенных обращений\n",
    "categories = [assign_category(text) for text in cleaned]\n",
    "\n",
    "# 4. Создать DataFrame из текста и категорий\n",
    "df = pd.DataFrame({\n",
    "    'text_clean': cleaned,\n",
    "    'category': categories\n",
    "})\n",
    "\n",
    "# 5. Сохранить DataFrame в CSV для последующего обучения модели\n",
    "df.to_csv('../data/processed/training_data.csv', index=False)\n",
    "\n",
    "print(\"Готово! Ваши данные разметились по категориям и сохранены в training_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca725a4-fe55-41d9-bf54-994793e7c4de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
